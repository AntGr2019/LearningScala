import org.apache.spark._
import org.apache.spark.SparkContext._

object WordsCount {
  def main(args: Array[String]) = {
    val sc = new SparkContext("local[*]", "wordcount")
    val data = sc.textFile("../ml-100k/simpledata.data")
    val result = data.flatMap(_.split(" ")).map(words => (words, 1)).reduceByKey(_ + _)
    result.collect.foreach(println)
  }
}

// (two,3)
// (one,4)
// (six,2)
// (three,2)
// (five,1)
// (four,4)
// (seven,3)
// ...
